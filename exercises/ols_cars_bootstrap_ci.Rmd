---
title: "More OLS w/ 'cars' lab"
author: "Lab author: "
date: "05/19/2020"
output: html_document
---

```{r setup, message=F}
library(tidyverse) # makes ggplot available and dplyr (data manipulation)
```



To get a feel for Ordinary Least Squares, let's use a simple data set: `cars`.  This is available in base R (you don't need to install any special package). The data are from, *Ezekiel, M. (1930) Methods of Correlation Analysis.* 

Search `cars` in the help tab.  What do the two variables capture? (See "Description") 

> your answer

What are the units for these two variables? meters, inches, feet? (Look at "Format" section)

> your answer

Execute the code. How many observations are in the data set?

> your answer

```{r inspect_the_data}
cars
```

Which variable do you think we should consider the "independent variable" and which the "dependent variable/response variable"? Why?

> 

Given your response above, should the x axis and y axis be assigned as they are below to keep with convention?  Or flipped? Flip them if you think this is needed; otherwise leave them as is.  Then replace the labels for the x and y axis  `labs(x = "a better label")` so they give more detail, and information on the units.  

```{r plotting}
cars %>% 
  ggplot() +
  aes(x = dist) +
  aes(y = speed) +
  geom_point()
```




What is the Pearson correlation coefficient for the relationship between these two variables? (run code below, and based on eyeballing figure)

>

Would you say it is strong or weak?

> 

```{r calculate_correlation_coefficient}
cor(cars$speed, cars$dist, method = "pearson")
```

Execute the code below.  The `geom_smooth` statement plots the *ordinary least squares* linear model fit for the data. 

> 

```{r plot_w_ols}
cars %>% 
  ggplot() +
  aes(x = speed) +
  aes(y = dist) +
  geom_point() +
  geom_smooth(method = lm, se = F)
```

---

Based on the output for the model (in the code chunk), below, write out the formula for this line (inside the double dolars sign -- for going into "math mode"); replace "Coefficient" and "Intercept" with the values you get back from model estimation `lm()`.  The greek letter "Epsilon" is standing in for model error (the predictions are not perfect --- we have residual error) . 


```{r model_estimation}
lm(dist ~ speed, data = cars)
```



$$ stopping\_distance = coefficient*speed + Intercept + \epsilon  $$

---

Given a car driving 20 miles per hour, what is your expectation (conditional mean) for stopping distance based on the linear model.  (Recall these are cars in the 1920s).  In other words, use the formula above to derive an expected value for stopping distance. You can use R as a calculator below:


```{r calculate_prediction}
# 20 * what? + what?
```

Also, what's the expected stopping distance for a car going 10 miles per hour as its start speed, given the result of linear modeling?

```{r}
# 10 * what? + what?
```


How how would you say that the default method for `geom_smooth` differ from when we set method to `lm` (just look at the result of executing the code below). 

>


```{r geomsmooth_default}
cars %>% 
  ggplot() +
  aes(x = speed) +
  aes(y = dist) +
  geom_point() +
  geom_smooth(se = F)
```

# Comparing statistical procedures

Executing the correlation *test* below, would you say that the relationship is statistically significant at the 95% confidence level (so comparing the p-value to an alpha threashold of .05).

> 

What is the numeric value of the p-value?

>

Interpreting this a little more, would you say that the relationship is *consistent with statistical independence*, or **not** *consistent with statistical independence*?  (Remember statistical independence means that knowing the value of one of your variables doesn't give you information about what the value of the other variable is likely to be.)

>  

```{r correlation_statistical_test}
cor.test(cars$speed, cars$dist)
```

Now look at the summary of statistics that we get from calculating the Ordinary Least Squares regression model, and asking for these statistics with "summary()".  Compare the statistics.  Does your p-value for the cor.test() appear here?

> 


```{r linear_model_summary}
lm(dist ~ speed, data = cars) %>% 
  summary()
```

In lecture, I said that the R-squared ("Multiple R squared" is a measure of goodness of fit of a model) is the pearson correlation coefficient squared.  Is this true?  

>

Test it out here:

```{r}
# replace "5" with the pearson correlation coefficient.
5^2
```






---
# Part II (a little less guided practice - but you can use the above code for reference. )

Okay. With the gapminder 2002 data (`gm_2002_europe` that is created below), create a scatter plot (geom_point) where life expectancy is the dependent variable, and gdp per capita is the independent variable.  Overlay a linear model on this plot.  Then, write out the formula for that line, after estimating the model using the lm() function (lm for linear model).  Using this formula, what's the expectation for life expectancy, (for a European country in 2002), that has an income of ten thousand dollars

```{r}
library(gapminder)  # load the data package.
gapminder %>% 
  filter(continent == "Europe") %>% 
  filter(year == 2002) ->
gm_2002_europe
```


```{r}
# scatterplot with OLS linear model overlayed
```

```{r}
# using lm() to get the values for the formula
```

Change out the formula below to one that reflects the linear model:

$$ x + y $$


---

# Part II

## Confidence intervals

Think about the cars experiment.  If we had done the experiment again, 3 days later, we might have gotten slightly different outcomes.  Another 3 days later, yet again slightly different outcomes.  Therefore we can expect variability in a linear OLS fit of these different potential realizations of the same experiment!  Hmmmm....

Above with our scatter plot, we just plotted the linear model fit - the line that minimized the sum of the squared residuals.  Now, let's reset se to T.  We get a confidence interval for our model.  


```{r see confidence intervals}
cars %>% 
  ggplot() +
  aes(x = speed) +
  aes(y = dist) +
  geom_point() +
  geom_smooth(method="lm", se=TRUE)
```


The confidence interval for the model is more about the *conditional mean*.  Where the mean for y -- *given a value of x* -- is expected.  

Above we looked at the classical analytic solution.  But let's also look at boot strapping confidence intervals!  

Bootstrapping is the technique of r___ with r____ to get an idea of the sampling variability -- the idea of what I might have seen had I drawn a different sample.  

More? Bradley Efron in "Bootstrap methods: another look at the jackknife" (1979)

Bootstrapping is powerful for when easy analytic solutions are not available -- finding the sampling distribution, and confidence intervals for the _____ for example, is much tricker than the sampling distribution for the median. 


```{r boot}
cars %>% 
  sample_frac(size = 1, replace = T) %>% 
  group_by(speed, dist) %>% 
  mutate(times_sampled = n()) %>% 
  ggplot() +
  aes(x = speed) +
  aes(y = dist) +
  coord_cartesian(xlim = c(0,30), ylim = c(0,100)) +
  geom_point(alpha = .2,
             size = 4) +
  geom_text(aes(label = times_sampled)) +
  geom_smooth(method = "lm", se = F)
```


So, how should I understand the grayed out confidence interval?  

> It holds a set of l_____ m_____ that is consistent with the observed sample. 



Note:  Another concept is the prediction interval -- where all any new data points to lie.  The confidence interval won't encompase so much of the data --- *rather is concerned with the conditional mean*.  


```{r}
lm_cars <- lm(dist ~ speed, data = cars)
predictions <- predict(lm_cars, interval = "prediction")

cars %>% 
  cbind(predictions) %>% 
  ggplot(data = .) +
    geom_point() +
    aes(x = speed) +
    aes(y = dist) +
    geom_smooth(method = "lm", se = T) +
    geom_line(aes(y = lwr), color = "red", linetype = "dashed") +
    geom_line(aes(y = upr), color = "red", linetype = "dashed")
```


Part III

# Causality


## Correlation does not imply _____


> If we plot x vs y ... we usually do think of the variable plotted on ___ as the explanatory variable, and the ___ variable as the outcome -- and something that ____ causes.

But... there are some pathways to observing a relationship between x and y.  What are a few? 

1. 
2.
3.


# What is the fundamental problem of causal inference?

$$ glucose_{joe} | Treatment $$


# How does randomization in Randomized control trials help establish causality? 




