---
title: Federalist Papers
author: Your name
output: html_document
---


The Federalist Papers are ready-to-use in corpus in [the R package "corpus", by Patrick O. Perry](https://CRAN.R-project.org/package=corpus).  

```{r setup, warning = F, message=FALSE}
library(tidyverse)
library(corpus)
library(tidytext)
```


# federalist data from Corpus package

The *federalist* dataset in the *corpus* package provides authors if known; but disputed authorship is designated NA. 

First a little cleaning.

```{r, }
federalist %>% 
  mutate(name = fct_inorder(name)) -> # we want these papers ordered, and not just alphabetically...
federalist
```



Below, we pipe federalist into head (which just returns 6 rows). What are the column names for the corpus package?  (Note, I set results='hide' in code chunck options because what is returned is a little overwhelming -- the entire texts of the federalist papers!)

> Answer here

```{r, results='hide'}
federalist %>%
  head()
```

Just a note. You might also like to use glimpse() as another way to look at the data (part of the `tibble` package in the tidyverse).

```{r, results='hide'}
federalist %>% 
  glimpse()
```

Based on the code *below*, how many **disputed** texts are there?  

> Answer

And how may did Jay write?

> Answer

```{r}
corpus::federalist %>% 
  group_by(author) %>% 
  tally() 
```

# Using tidytext package

What is the "corpus" used as an example in section 1.3 in the TidyText package website: www.tidytextmining.com (Links to an external site.) Links to an external site.

> Answer

# Unnesting from tidytext

*Below*, describe what `unnest_tokens` does.

> Answer

And how many rows result from the transformation?

> Answer

Change the `token` type to *"sentences"* below from the default *"words"*.  Now how many rows result?

> Answer

```{r}
library(tidytext)
corpus::federalist %>% 
  select(name, author, text) %>% 
  unnest_tokens(output = word, 
                input = text, 
                token = "words")
```

Look at one more variant.  What is an ngram?

> Your answer...

Also, take a minute to check out google's ngram viewer. https://books.google.com/ngrams What does it do?

> Your answer.  

Search for a phrase related to international relations online at https://books.google.com/ngrams, report it below and describe the trend:

> Your answer. 

```{r, results='hide'}
corpus::federalist %>% 
  select(name, author, text) %>% 
  tidytext::unnest_tokens(output = word, 
                input = text, 
                token = "ngrams", 
                n = 3)
```

---

# Texts mentioning "Foreign"

Now we focus on a subset of texts.  Of the 85 Federalist papers, *how many contain the text foreign or Foreign*?  (We are using the regular expressions function str_detect, which returns TRUE if foreign or Foreign are found)

> Your answer...

Now modify the code below to return only the cases where foreign or Foreign are found in the title.  How many Federalist papers have Foreign in the title? (change the column that is the input for `str_detect`, right now it is the column `text`, the 6th column in the data frame)

> Your answer...

Besides "foreign" what other words might be in a "dictionary" of words related to international relations.  List a few below.

> Your answer

```{r, results='hide'}
corpus::federalist %>% 
  filter(str_detect(text, "foreign|Foreign"))
```


# Counting words

*Below, add comments to the code describing what each line of the pipeline does.* You may highlight the code partially and execute to clarify what's going on if you need to.  Check in with your group members to make sure they also understand the pipeline.

Which are the 5 word that occurs the most frequently in all the Federalist Papers texts?

> Answer

Now. Pipe the data into ggplot(). Then create a histogram (`geom_histogram()`), where `rank` is mapped to the x position, and `times_in_federalist` is mapped to the y position. Adjust the scale of x to log10.

Describe the distribution of the occurance of words:

> Your answer

Search for Zipf's law on the internet.  Copy a definition here:

> 

Is the pattern you  you observe consistent with Zipfs law?

> Your answer

```{r, results='hide'}
corpus::federalist %>% 
  select(text, author, name) %>% 
  tidytext::unnest_tokens(output = word, input = text) %>% 
  group_by(word) %>% 
  tally() %>% 
  arrange(-n) %>% 
  rename(times_in_federalist = n) %>% 
  mutate(rank = 1:n())
```

We can also make a column plot with the top several.  We use slice() to grab rows 1 to 10.  Then pipe into ggplot().  The code below is already written for this purpose.  *Add some notes with the comment character to explain the steps of the code below.  Quiz your group members.  Use partial highlighting to help clarify the steps.*

If you were using text analysis vocabulary, how would you describe the words that make the top ten most frequent in the plot?  "They are ... words."

> 

```{r}
corpus::federalist %>% 
  select(text, author, name) %>% 
  tidytext::unnest_tokens(output = word, input = text) %>% 
  group_by(word) %>% 
  tally() %>% 
  arrange(-n) %>% 
  rename(times_in_federalist = n) %>% 
  slice(1:10) %>% 
  ggplot() +
  aes(x = reorder(word, times_in_federalist)) +
  aes(y = times_in_federalist) +
  geom_col() +
  coord_flip() 
```




## Stop words

How many rows (words) are in the stop_words data frame (execute the code below)?

> answer

In fact, as explained by the tidytext package website, "dataset in the tidytext package contains stop words from three lexicons."  (i.e. dictionaries of words that are considered "stop words" by three different groups.)  "SMART" is the first one.  What is another?  Click down (way down!) through the rows of data to answer the question. 

> answer.


```{r, results='hide'}
tidytext::stop_words 
```

The anti_join funtion is one of the join functions from dplyr (like full join or inner join). It keeps "left hand" data where case is not found in "right hand" data.

What are the first two rows () that are removed via the anti_join with stopwords. You might partially execute the code w/o the anti_join statement to figure this out. 

> answer

```{r, results='hide'}
corpus::federalist %>% 
  select(text, author, name) %>% 
  tidytext::unnest_tokens(output = word, input = text) %>% 
  anti_join(stop_words)  # natural join is on columns "word" 
                         # from transformation of federalist data
                         # and from stop_words
```

After removing stopwords, what do you *think* might be the two most frequently observed word in the corpus?  

> answer

*Now add to this pipeline below to answer this question*.   What are indeed the two most frequently observed word in the corpus?  (You may refer to code above to get this done!  We did something similar w/o removing stop words before.  The result will be a column of words, and a column with count, "n")

In words: 1) You need to *group by* `word`.  2) then you can *tally* up the number of times the word is observed 3) then you can *arrange* the data so that you see the words occuring the most at the of your list.  

>

```{r, results='hide'}
corpus::federalist %>% 
  select(text, author, name) %>% 
  tidytext::unnest_tokens(output = word, input = text) %>% 
  anti_join(tidytext::stop_words)  # Add to the pipeline here...
```

Below, produce a ggplot with the frequencies of the 10 most frequent words in the federalist papers that are *NOT* stop words.  (Refer to code above if needed.)

```{r}
# geom_col() plot - columns representing frequency.

```




# Relationships between texts **(Return point - you will return to this point later)**

What is the most frequently occuring word -- that is not a stop word -- in any single document? Which text does it occur in?

> answer

```{r, results='hide'}
corpus::federalist %>% 
  select(name, text, title) %>% 
  tidytext::unnest_tokens(output = word, input = text) %>% 
  anti_join(tidytext::stop_words) %>%
  group_by(name, word) %>% 
  count() %>% 
  arrange(-n) %>% 
  ungroup() ->
federalist_doc_term_frequency

federalist_doc_term_frequency
```

The following code takes the correlation for the count of tokens between sets of two texts.  This is similar to cosign similarity that we discussed in class.  It is a single statistic that measures the similarity between any pair of texts.  How many pairs of texts are there? 
> your answer

```{r, results='hide'}
federalist_doc_term_frequency %>% 
  widyr::pairwise_cor(item = name,
                      feature = word, 
                      value = n, 
                      sort = TRUE) ->
document_correlation

document_correlation
```

We can look at the strength of the correlation as a heat map, as seen below.  What color represents more similar texts?  

> 

Chat in your group about how to interpret this plot. Why are their white tiles on the diagonal? 

>

Name a pair of texts that have a high word correlation with one another:

>

Name a pair that have low correlation with each other:

>


```{r, fig.height=20}
document_correlation %>% 
  ggplot() +
  aes(x = item1) +
  aes(y = item2) +
  aes(fill = correlation) +
  geom_tile() +
  scale_fill_viridis_c(direction = -1) +
  theme(axis.text.x = element_text(angle = 90)) +
  theme_bw(base_size = 20) +
  theme(axis.text.x = element_text(angle = 90)) +
  theme(aspect.ratio = 1)
```


---

Done with Part I!
 
---



