---
title: "ols"
subtitle: "made with flipbookr and xaringan"
author: "Gina Reynolds, Jan 2020"
output:
  xaringan::moon_reader:
    chakra: libs/remark-latest.min.js
    lib_dir: libs
    css: [default, hygge, ninjutsu]
    nature:
      ratio: 16:10
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---





```{r, include = F}
library(tidyverse)
library(flipbookr)
knitr::opts_chunk$set(cache = F, comment = "")
```

---

`r flipbookr::chunk_reveal("basic", break_type = 1)`


```{r basic, include = F}
cars %>% 
  ggplot() +
  labs(title = "Fitting an OLS model to data in ggplot2") +
  aes(x = speed) +
  aes(y = dist) +
  geom_point(color = "steelblue",
             size = 2) +
  geom_smooth(method = lm, se = F)
```

---

OLS in base R


---

`r flipbookr::chunk_reveal("base_ols")`

```{r base_ols, include = F}
# fitting a model
lm(formula = dist ~ speed,
   data = cars) ->
cars_model
# using summary() to
# to pull out more info
cars_model %>% 
  summary() %>% 
  .$residuals
```

---

# But {broom} is cool ...

--

...anticipating that we might want lots of this info pulled out systematically as data frames


--

Broom returns dataframes at 3 levels

- broom::glance(), overall model statistics
- broom::tidy(), covariate level model statistics
- broom::augment(), observation level statistics

---

# *Model* level stats

```{r}
cars_model %>% 
  broom::glance()
```


---

# *Covariate* level stats


```{r}
cars_model %>% 
  broom::tidy()
```

---

# *Observation* level


```{r}
cars_model %>% 
  broom::augment() 
```

---

`broom::augment()` is good for plotting those residuals and fitted values, and checking model assumptions.

---

`r flipbookr::chunk_reveal("augment")`


```{r augment, include = F}
cars_model %>% 
  broom::augment() %>% 
  ggplot() +
  labs(title = "Plot of model fit") +
  aes(x = speed) +
  aes(y = dist) +
  geom_point(col = "steelblue") +
  geom_smooth(method = lm, se = F) +
  geom_point(aes(y = .fitted)) +
  aes(xend = speed) +
  aes(yend = .fitted) +
  geom_segment(color = "red", 
               linetype = "dashed") +
  # input value of 21 miles per hour
  geom_vline(xintercept = 21,
             linetype = "dotted") +
  # yields predicted for y, stopping distance
  geom_hline(yintercept = 
               predict(cars_model, 
                       data.frame(speed = 21)),
             linetype = "dotted")
```

---

Checking assumption:

residuals not correlated with predictions...

---

`r flipbookr::chunk_reveal("residuals_plot")`


```{r residuals_plot, include = F}
cars_model %>% 
  broom::augment() %>%
  ggplot() +
  labs(title = "Residuals v. Fitted") +
  aes(x = .fitted) +
  aes(y = .resid) +
  geom_point(col = "steelblue") +
  geom_hline(yintercept = 0,
             lty = "dashed") +
  geom_smooth(se = F, span = 1, 
              color = "red")
```


---

# Checking for outliers with Cook's distance


---

`r flipbookr::chunk_reveal("cooks")`

```{r cooks, include = F}
cars_model %>% 
  broom::augment() %>%
  mutate(id = row_number()) %>% 
  ggplot() +
  labs(title = "Cook's Distance, by ID (row number)") +
  labs(subtitle = "Cook's Distance, a measure of influence via a leave one out procedure") +
  aes(x = id) +
  aes(y = .cooksd) +
  geom_point() +
  aes(label = id) +
  geom_text(check_overlap = T, 
            nudge_y = .015)
```

---


```{r, echo = F, eval = F}
cars_model %>% 
  broom::augment() %>%
  mutate(id = row_number()) %>% 
  ggplot() +
  labs(title = "Cook's Distance v. standardized residual") +
  labs(subtitle = "Cook's Distance, a measure of influence via a leave one out procedure") +
  aes(x = id) +
  labs(x = "id (dataframe row number)") +
  aes(y = .cooksd) +
  geom_point() +
  aes(label = id) +
  geom_text(check_overlap = T, 
            nudge_y = .015)
```

---

Are the residuals normally distributed? 


---

`r flipbookr::chunk_reveal("qqplot")`

```{r qqplot, include = F}
cars_model %>% 
  broom::augment() %>% 
  mutate(expected = qnorm((1:n() - .5)/n())) %>% 
  ggplot() +
  labs(title = "Normal q-q plot") +
  aes(y = sort(.std.resid)) +
  aes(x = expected) +
  geom_rug() +
  geom_point() +
  coord_equal(ratio = 1, 
              xlim = c(-4, 4),
              ylim = c(-4, 4)) +
  # add line of equivilance
  geom_abline(intercept = 0, 
              slope = 1, 
              linetype = "dotted")
```


---

# 


```{r}
cars_model
```




---



```{r, eval = F, echo = F}
cars_model %>% 
  plot()
```

---

# multiple regression...

--
...and multiple models in a single table

---

# subsetting to 2002

```{r}
library(gapminder)
gapminder %>% 
  filter(year == 2002) ->
gapminder_2002
```

---

`r flipbookr::chunk_reveal("multi_model")`


```{r multi_model, include = F}
lm(lifeExp ~ gdpPercap, 
   data = gapminder_2002) ->
model1
lm(lifeExp ~ gdpPercap + pop, 
   data = gapminder_2002) ->
model2
lm(lifeExp ~ gdpPercap + 
     continent, 
   data = gapminder_2002) ->
model3
#
stargazer::stargazer(
  model1, model2, model3,
  # if you want to work interactivevly, change "html" to "text" 
  # note, chunk results option is "asis"
  type = "text",  
  dep.var.labels = "Life Expectancy (years)", 
  covariate.labels = c("GDP per cap (Thousands $US)", 
                       "Population (Millions)"), 
  style = "qje", 
  font.size = "small",
  title = "Models of Life Expectancy in 2002") ->
formatted_models_table
```


---

if you change they `type` argument to "html", and the code chunk option to results = "asis"
 
```{r, results = "asis", echo = F}
stargazer::stargazer(
  model1, model2, model3,
  # if you want to work interactivevly, change "html" to "text" 
  # note, chunk results option is "asis"
  type = "html",  
  dep.var.labels = "Life Expectancy (years)", 
  covariate.labels = c("GDP per cap (Thousands $US)", 
                       "Population (Millions)"), 
  style = "qje", 
  font.size = "small",
  title = "Models of Life Expectancy in 2002")
```




---

# the maths!

---

`r chunk_reveal("automatically")`

```{r automatically, include = F}
# create artificial data set
#  set seed for reproducibility
set.seed(501)
 
#  head_size in cm
rnorm(n = 20, mean = 30, sd = 10) ->
  head_size
 
#  tail_length in kilogram the relationship between head_size
#  and tail_length is completely random
head_size *
  rnorm(20, 0, sd = 15) + 
  80  ->
tail_length
 
#  join head_size and tail_length in data frame
data.frame(head_size, tail_length) ->
  df

lm(tail_length ~ head_size, data = df) %>% 
  summary() ->
reg
```

---

beta = ((X'X)^(-1))X'y
res = y - beta1 - beta2*X2
VCV = (1/(n-k))res'res(X'X)^(-1)

---

`r chunk_reveal("manual")`



```{r manual, include = F}
## build OLS estimator manually
 
#  define X matrix and y vector
cbind(1, df$head_size) %>% 
  as.matrix() -> 
  X 
as.matrix(df$tail_length) ->
  y
 
#  estimate the coeficients beta
t(X) %*% X %>% 
solve() %*% 
  t(X) %*% 
  y ->
  beta
 
## calculate residuals
y - 
  beta[1] - 
  (beta[2] * X[,2]) %>% 
as.matrix() ->
  res

## define the number of observations and the number of
#  parameters 
nrow(df) -> 
  n

ncol(X) ->
  k
 
## calculate the Variance-Covariance matrix VCV

1/(n - k) * 
  as.numeric(t(res) %*% res) * 
  solve(t(X) %*% X) ->
VCV
 
## calculate standard errors se of coefficients
diag(VCV) %>% 
  sqrt() ->
se
 
## calculate the p-values
rbind(2*pt(abs(beta[1]/se[1]), df = n - k,
                      lower.tail = FALSE),
                 2*pt(abs(beta[2]/se[2]), df = n - k,
                      lower.tail = FALSE)) ->
  p_value
 
## combine all necessary information
as.data.frame(cbind(c("(Intercept)","head_size"),
                                beta,se,p_value)) ->
  output
c("Coefficients:","Estimate", 
                   "Std. Error","Pr(>|t|)") ->
  names(output)
 
#compare automatic lm and manual output
output
summary(reg)
```



```{css, eval = TRUE, echo = F}
.remark-code{line-head_size: 1.5; font-size: 80%}
```
